<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-岭回归 LASSO回归" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/29/%E5%B2%AD%E5%9B%9E%E5%BD%92%20LASSO%E5%9B%9E%E5%BD%92/" class="article-date">
  <time datetime="2020-07-29T11:02:50.570Z" itemprop="datePublished">2020-07-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h1><p>机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm 和ℓ2-norm，中文称作 L1正则化 和 L2正则化，或者 <strong>L1范数</strong> 和 <strong>L2范数</strong>。<br>        L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。下图是Python中Lasso回归的损失函数，式中加号后面一项<img src="/.com//ML\images\8.png" alt="8"> 即为L1正则化项。</p>
<p>![9](岭回归 LASSO回归\9.png)</p>
<p>下图是Python中Ridge回归的损失函数，式中加号后面一项![10](岭回归 LASSO回归\10.png)</p>
<p>![11](岭回归 LASSO回归\11.png)</p>
<p>一般回归分析中ω表示特征的系数，从上式可以看到正则化项是对系数做了处理（限制）。<strong>L1正则化和L2正则化的说明如下：</strong></p>
<ul>
<li>L1正则化是指权值向量ω中各个元素的绝对值之和，通常表示为 ![12](岭回归 LASSO回归\12.png)</li>
<li>L2正则化是指权值向量ω中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为![13](岭回归 LASSO回归\13.png)</li>
</ul>
<p>一般都会在正则化项之前添加一个系数，Python的机器学习包sklearn中用α表示，一些文章也用λ表示。这个系数需要用户指定。那添加L1和L2正则化有什么用？下面是L1正则化和L2正则化的作用，这些表述可以在很多文章中找到。</p>
<ul>
<li>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择</li>
<li>L2正则化可以防止模型过拟合（overfitting），一定程度上，L1也可以防止过拟合</li>
</ul>
<h2 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h2><p>这部分内容将解释<strong>为什么L1正则化可以产生稀疏模型（L1是怎么让系数等于零的）</strong></p>
<h3 id="稀疏模型与特征选择的关系"><a href="#稀疏模型与特征选择的关系" class="headerlink" title="稀疏模型与特征选择的关系"></a>稀疏模型与特征选择的关系</h3><p>上面提到L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵？<br>稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。</p>
<h3 id="L1正则化和特征选择的关系"><a href="#L1正则化和特征选择的关系" class="headerlink" title="L1正则化和特征选择的关系"></a>L1正则化和特征选择的关系</h3><p>特征选择( Feature Selection )也称特征子集选择( Feature Subset Selection , FSS )，或属性选择( Attribute Selection )。是指<strong>从已有的M个特征(Feature)中选择N个特征</strong>使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以<strong>降低数据集维度</strong>的过程,是提高学习算法性能的一个重要手段</p>
<h4 id="L1正则化正则化可以产生稀疏模型"><a href="#L1正则化正则化可以产生稀疏模型" class="headerlink" title="L1正则化正则化可以产生稀疏模型"></a>L1正则化正则化可以产生稀疏模型</h4><p>假设有如下带L1正则化的损失函数：</p>
<img src="/.com//14png.png" alt="14png" style="zoom:80%;">

<p>其中J0是原始的损失函数，加号后面的一项是L1正则化项，α是正则化系数。注意到L1正则化是权值的绝对值之和，J是带有绝对值符号的函数，因此J是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数 J<del>0</del>后添加L1正则化项时，相当于对 J<del>0</del>做了一个约束。令<img src="/.com//ML\images\17.png" alt="17">，则<img src="/.com//ML\images\18.png" alt="18">，此时我们的任务变成在L约束下求出 J<del>0</del>取最小值的解。考虑二维的情况，即只有两个权值ω^1^和ω^2^，此时L=∣ω^1^∣+∣ω^2^∣。对于梯度下降法，求解 J<del>0</del>的过程可以画出等值线，同时L1正则化的函数L也可以在ω^1^ω^2^的二维平面上画出来。如下图：</p>
<p>此图只是三维空间的一个截面</p>
<img src="/.com//15.png" alt="15" style="zoom: 80%;">



<p>图中等值线是 J<del>0</del>的等值线，黑色方形是L函数的图形。L=|ω^1^|+|ω^2^|，这个函数画出来就是一个方框。<br>        在图中，当  J<del>0</del>等值线与L图形首次相交的地方就是最优解。上图中  J<del>0</del>与L在L 的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是(ω^1^,ω^2^)=(0,ω)。可以直观想象，因为L 函数有很多『突出的角』（二维情况下四个，多维情况下更多），<strong>J<del>0</del>与这些角接触的机率会远大于与L其它部位接触的机率</strong>（这是很直觉的想象，突出的角比直线的边离等值线更近写），<strong>而在这些角上，会有很多权值等于0</strong>（因为角就在坐标轴上），这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。<br>        而正则化前面的系数α，可以控制L图形的大小。<strong>α越大</strong>，L的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优点的值(ω1,ω2)=(0,ω)中的<strong>ω可以取到很小的值</strong>；<strong>α越小</strong>，L的图形越大（上图中的黑色方框）<strong>ω可以取到很大的值</strong></p>
<h4 id="L2正则化不可以产生稀疏模型"><a href="#L2正则化不可以产生稀疏模型" class="headerlink" title="L2正则化不可以产生稀疏模型"></a>L2正则化不可以产生稀疏模型</h4><p>假设有如下带L2正则化的损失函数：</p>
<img src="/.com//21.png" alt="21" style="zoom:80%;">



<p>同样可以画出他们在二维平面上的图形，如下：</p>
<p>![22](岭回归 LASSO回归\22.png)</p>
<p>二维平面下L2正则化的函数图形是个圆（绝对值的平方和，是个圆），与方形相比，被磨去了棱角。因此J0与L相交时使得ω^1^或ω^2^等于零的机率小了许多（这个也是一个很直观的想象），这就是为什么L2正则化不具有稀疏性的原因，因为不太可能出现多数ω都为0的情况。</p>
<h3 id="L1正则化参数"><a href="#L1正则化参数" class="headerlink" title="L1正则化参数"></a>L1正则化参数</h3><p>通常越大的λ可以让代价函数在参数为0时取到最小值。因为正则化系数越大，正则化的函数图形（上文图中的方形或圆形）会向坐标轴原点收缩得越厉害，这个现象称为shrinkage，过程可以称为shrink to zero. 下面是一个简单的例子。</p>
<p>假设有如下带L1正则化项的代价函数：</p>
<p>![19](岭回归 LASSO回归\19.png)</p>
<p>其中x是要估计的参数，相当于上文中提到的ω以及θ. 这个例子中的正则化函数L就是L=λ∣x∣。注意到L1正则化在某些位置是不可导的，当λ足够大时可以使得F(x)在x=0时取到最小值。如下图：</p>
<p>预测值y=x，真实值y=1， x为参数，不是自变量，只预测了一个值</p>
<p>![20](岭回归 LASSO回归\20.png)</p>
<p>作为一个直观的例子，这个图的示例中，取了f(x)=(x−1)^2^作为损失函数，其实可以取更复杂的，但不好画图，不过原理是一样的，因为损失函数都是凸函数，很多性质是一样的。正则化分别取λ=0.5和λ=2，可以看到越大的λ越容易使F(x)在x=0时取到最小值。此外也可以自己计算一下，当损失函数f(x)和正则化函数L=∣x∣在定义域内第一次相交的地方，就是整个代价函数F(x)的最优解。</p>
<h2 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h2><h3 id="L2正则化和过拟合的关系"><a href="#L2正则化和过拟合的关系" class="headerlink" title="L2正则化和过拟合的关系"></a>L2正则化和过拟合的关系</h3><p>拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。</p>
<p>那为什么L2正则化可以获得值很小的参数？<br>        以线性回归中的梯度下降法为例，使用Andrew Ng机器学习的参数表示方法。假设要求解的参数为θ，h<del>θ</del>(x)是我们的假设函数。线性回归一般使用平方差损失函数。单个样本的平方差是(h<del>θ</del>(x)−y)^2^，如果考虑所有样本，损失函数是对每个样本的平方差求和，假设有m个样本，线性回归的代价函数如下，为了后续处理方便，乘以一个常数<img src="/.com//ML\images\23.png" alt="23">：</p>
<p>![24](岭回归 LASSO回归\24.png)</p>
<h3 id="L2正则化参数"><a href="#L2正则化参数" class="headerlink" title="L2正则化参数"></a>L2正则化参数</h3><p>从公式5可以看到，λ越大，θ<del>j</del>衰减得越快。另一个理解可以参考图2，λ越大，L2圆的半径越小，最后求得代价函数最值时各参数也会变得很小，同样是一个shrink to zero的过程，原理与L1正则化类似。</p>
<h1 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h1><p>在线性回归模型中，其参数估计公式为β=(X^T^X) ^−1^ X^T^y, 当X^T^X越趋近于0，会使得回归系数趋向于无穷大，此时得到的回归系数是无意义的。解决这类问题可以使用岭回归和LASSO回归，主要针对自变量之间存在多重共线性或者自变量个数多于样本量的情况。</p>
<p><strong>岭回归所解决的问题：</strong></p>
<p>（1）数据样本数比特征数少的情况，矩阵的逆不能直接计算，当求n元一次方程组时，需要有n个方程才可以求出唯一解，也            就是说最少需要n个样本。</p>
<p>（2）即使样本数多于特征数，若特征高度相关，X^T^X的逆依然无法计算。</p>
<p>   (3)  过拟合</p>
<h3 id="1-参数推导"><a href="#1-参数推导" class="headerlink" title="1.参数推导"></a><strong>1</strong>.参数推导</h3><p>线性回归模型的损失函数</p>
<p>​                        J(β) = ∑(y-Xβ)^2^</p>
<p>为了保证回归系数β可求，岭回归模型在损失函数上加了一个<strong>L2范数</strong>的惩罚项</p>
<p>​                        J*(*β) = ∑(y-Xβ)^2^+λ∑β^2^</p>
<p><strong>推导过程：</strong>            </p>
<p>![1](岭回归 LASSO回归\1.png)</p>
<p>L2范数惩罚项的加入使得 (X^T^X+λI) 满秩，保证了可逆，但是也由于惩罚项的加入，使得回归系数β的估计不再是无偏估计。所以岭回归是以放弃无偏性、降低精度为代价解决病态矩阵问题的回归方法。单位矩阵I的对角线上全是1，像一条山岭一样，这也是岭回归名称的由来。</p>
<h3 id="2-λ的选择"><a href="#2-λ的选择" class="headerlink" title="2.λ的选择"></a>2.<em>λ</em>的选择</h3><img src="/.com//2.png" alt="2" style="zoom:;">

<p>上图公式中平方应在括号外</p>
<p>![3](岭回归 LASSO回归\3.png)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data=pd.read_excel(<span class="string">r&#x27;C:\Users\Administrator\Desktop\diabetes.xlsx&#x27;</span>)</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">predictors=data.columns[:<span class="number">-1</span>]</span><br><span class="line">x_train,x_test,y_train,y_test=model_selection.train_test_split(data[predictors],data.Y,</span><br><span class="line">                                                               test_size=<span class="number">0.2</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#构造不同的lambda值</span></span><br><span class="line">Lambdas=np.logspace(<span class="number">-5</span>,<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment">#存放偏回归系数</span></span><br><span class="line">ridge_cofficients=[]</span><br><span class="line"><span class="keyword">for</span> Lambda <span class="keyword">in</span> Lambdas:</span><br><span class="line">    ridge=Ridge(alpha=Lambda,normalize=<span class="literal">True</span>)</span><br><span class="line">    ridge.fit(x_train,y_train)</span><br><span class="line">    ridge_cofficients.append(ridge.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制岭迹曲线</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;Microsoft YaHei&#x27;</span>]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line">plt.style.use(<span class="string">&#x27;ggplot&#x27;</span>)</span><br><span class="line">plt.plot(Lambdas,ridge_cofficients)</span><br><span class="line"><span class="comment">#x轴做对数处理</span></span><br><span class="line">plt.xscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Cofficients&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<p><img src="/.com//ML\images\4.png" alt="4"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"></span><br><span class="line">data=pd.read_excel(<span class="string">r&#x27;C:\Users\Administrator\Desktop\diabetes.xlsx&#x27;</span>)</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">predictors=data.columns[:<span class="number">-1</span>]</span><br><span class="line">x_train,x_test,y_train,y_test=model_selection.train_test_split(data[predictors],data.Y,</span><br><span class="line">                                                               test_size=<span class="number">0.2</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#构造不同的lambda值</span></span><br><span class="line">Lambdas=np.logspace(<span class="number">-5</span>,<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment">#设置交叉验证的参数，使用均方误差评估</span></span><br><span class="line">ridge_cv=RidgeCV(alphas=Lambdas,normalize=<span class="literal">True</span>,scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>,cv=<span class="number">10</span>)</span><br><span class="line">ridge_cv.fit(x_train,y_train)</span><br><span class="line">print(ridge_cv.alpha_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<h1 id="LASSO回归"><a href="#LASSO回归" class="headerlink" title="LASSO回归"></a>LASSO回归</h1><p><strong>用处</strong>：LASSO回归可以实现<strong>特征选择</strong>，从已有的M个特征中选择N个特征使得系统的特定指标最优化，去掉对结果没有影响的变量，是从原始特征中选择出一些最有效特征以降低数据集维度的过程,是提高学习算法性能的一个重要手段。一定程度上可以<strong>防止过拟合</strong>。</p>
<h3 id="1-参数推导-1"><a href="#1-参数推导-1" class="headerlink" title="1.参数推导"></a>1.参数推导</h3><img src="/.com//5.png" alt="5" style="zoom:150%;">

<p><img src="/.com//ML\images\6.png" alt="6"></p>
<h3 id="2-λ的选择-1"><a href="#2-λ的选择-1" class="headerlink" title="2.λ的选择"></a>2.λ的选择</h3><p>直接使用交叉验证法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"></span><br><span class="line">data=pd.read_excel(<span class="string">r&#x27;C:\Users\Administrator\Desktop\diabetes.xlsx&#x27;</span>)</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">predictors=data.columns[:<span class="number">-1</span>]</span><br><span class="line">x_train,x_test,y_train,y_test=model_selection.train_test_split(data[predictors],data.Y,</span><br><span class="line">                                                               test_size=<span class="number">0.2</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#构造不同的lambda值</span></span><br><span class="line">Lambdas=np.logspace(<span class="number">-5</span>,<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment">#设置交叉验证的参数，使用均方误差评估</span></span><br><span class="line">lasso_cv=LassoCV(alphas=Lambdas,normalize=<span class="literal">True</span>,cv=<span class="number">10</span>,max_iter=<span class="number">10000</span>)</span><br><span class="line">lasso_cv.fit(x_train,y_train)</span><br><span class="line">print(lasso_cv.alpha_)</span><br></pre></td></tr></table></figure>




      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/29/%E5%B2%AD%E5%9B%9E%E5%BD%92%20LASSO%E5%9B%9E%E5%BD%92/" data-id="ckd79u58w0001bkdz8twv27bd" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-广义线性模型" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/29/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2020-07-29T10:45:12.306Z" itemprop="datePublished">2020-07-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h1><h2 id="普通最小二乘法"><a href="#普通最小二乘法" class="headerlink" title="普通最小二乘法"></a>普通最小二乘法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line">reg = linear_model.LinearRegression()</span><br><span class="line">reg.fit([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># y=x+1</span></span><br><span class="line">print(reg.coef_)  <span class="comment"># 参数值</span></span><br><span class="line">print(reg.intercept_)  <span class="comment"># 截距</span></span><br><span class="line">print(reg.predict([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]]))  <span class="comment"># 想要预测的数据，放到一个列表中</span></span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[<span class="number">0.5</span> <span class="number">0.5</span>]</span><br><span class="line"><span class="number">5.551115123125783e-17</span></span><br><span class="line">[<span class="number">1.5</span> <span class="number">2.5</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LinearRegression(copy_X=<span class="literal">True</span>, fit_intercept=<span class="literal">True</span>, n_jobs=<span class="number">1</span>, normalize=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>fit_intercept：bool, default=True</p>
<p>是否计算截距</p>
</li>
<li><p><strong>normalize</strong>：bool, default=False</p>
<p>是否将数据标准化</p>
</li>
<li><p><strong>copy_X</strong>：bool, default=True</p>
<p>如果为True，将复制X；否则为X。否则，它可能会被覆盖。</p>
</li>
<li><p><strong>n_jobs</strong>：int, default=None</p>
<p>使用进程的数量，与电脑的CPU有关，-1表示使用所有处理器</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fit(self, X, y, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>X是自变量，y是每个自变量所对应的因变量，sample_weight是每个自变量的权重</p>
<h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line">reg = linear_model.Ridge(alpha=<span class="number">.5</span>)  <span class="comment"># alpha参数大于0</span></span><br><span class="line">reg.fit([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]], [<span class="number">0</span>, <span class="number">.1</span>, <span class="number">1</span>])</span><br><span class="line">print(reg.coef_, reg.intercept_, reg.predict([[<span class="number">3</span>, <span class="number">3</span>]]))</span><br></pre></td></tr></table></figure>

<p>除了创建时不同，与其他线性模型一样， <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge"><code>Ridge</code></a> 用 <code>fit</code> 方法完成拟合，并将模型系数 ω 存储在其 <code>coef_</code> 成员中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ridge(alpha=<span class="number">1.0</span>, fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, copy_X=<span class="literal">True</span>, max_iter=<span class="literal">None</span>, tol=<span class="number">0.001</span>, solver=<span class="string">&#x27;auto&#x27;</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>alpha</strong>：float, ndarray of shape (n_targets), default=1.0</p>
<p>正则化参数，传入的是正浮点数，如果有多个惩罚项则传入数组</p>
</li>
<li><p><strong>fit_intercept</strong>：boolean, optional, default True</p>
<p>是否计算截距。如果为False，对数据进行去中心化处理。</p>
</li>
<li><p><strong>normalize</strong>：boolean, optional, default False</p>
<p>对数据X进行标准化</p>
</li>
<li><p><strong>max_iter</strong>：int, default=None</p>
<p>共轭梯度求解器的最大迭代次数。</p>
</li>
<li><p><strong>tol：</strong>float, optional</p>
</li>
</ul>
<p>　　优化容忍度：如果更新大于tol，继续优化，直到小于tol。</p>
<ul>
<li><p><strong>solver</strong>：{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}, default=’auto’</p>
<p>参数计算方式</p>
<p>auto：根据数据类型自动选择求解器。</p>
<p>svd：使用X的奇异值分解来计算Ridge系数，对于奇异矩阵，比cholesky更稳定。</p>
<p>cholesky：使用标准的scipy.linalg.solve函数来获取封闭形式的解决方案。</p>
<p>sparse_cg：使用scipy.sparse.linalg.cg中的共轭梯度求解器。作为一种迭代算法，对于大规模数据（可以设置tol和max_iter），此求解器比cholesky更合适。</p>
<p> lsqr：使用专用的正则化最小二乘例程scipy.sparse.linalg.lsqr。它是最快的，并且使用迭代过程。</p>
<p> sag和saga：sag使用随机平均梯度下降，saga使用经过改进的无偏版本SAGA。两种方法都使用迭代过程，并且当n_samples和n_features都较大时，通常比其他求解器更快。请注意，只有在比例大致相同的要素上才能确保“ sag”和“ saga”快速收敛。您可以使用sklearn.preprocessing中的缩放器对数据进行预处理。</p>
</li>
<li><p><strong>random_state：</strong>int, RandomState instance or None, optional, default None</p>
<p>伪随机数发生器种子，随机选择特征来更新模型。如果为int，random_state即为随机数发生器使用的种子；如果为RandomState实例，random_state即为随机数发生器；如果为None，随机数发生器为np.random使用的随机数发生器实例。该参数仅当selection=‘random’时使用。</p>
</li>
</ul>
<h5 id="岭回归所解决的问题："><a href="#岭回归所解决的问题：" class="headerlink" title="岭回归所解决的问题："></a><strong>岭回归所解决的问题：</strong></h5><p>普通最小二乘法会在实际应用中遇到以下问题：</p>
<p>（1）数据样本数比特征数少的情况，矩阵的逆不能直接计算，当求n元一次方程组时，需要有n个方程才可以求出唯一解，也就是说最少需要n个样本。</p>
<p>（2）即使样本数多于特征数，若特征高度相关，X^T^X的逆依然无法计算。</p>
<p>   (3)  过拟合</p>
<h5 id="设置正则化参数：广义交叉验证"><a href="#设置正则化参数：广义交叉验证" class="headerlink" title="设置正则化参数：广义交叉验证"></a>设置正则化参数：广义交叉验证</h5><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV"><code>RidgeCV</code></a> 通过内置的关于的 alpha 参数的交叉验证来实现岭回归。 </p>
<p>默认为 Generalized Cross-Validation(广义交叉验证 GCV)，这是一种有效的留一验证方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">reg = linear_model.RidgeCV(alphas=[<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>])</span><br><span class="line">reg.fit([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]], [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">1</span>])</span><br><span class="line">print(reg.alpha_)  <span class="comment"># 得出的正则化参数</span></span><br></pre></td></tr></table></figure>

<p>指定cv属性的值将触发(通过GridSearchCV的)交叉验证。例如，cv=10将触发10折的交叉验证，而不是广义交叉验证(GCV)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RidgeCV(alphas=(<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>), fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, scoring=<span class="literal">None</span>, cv=<span class="literal">None</span>, gcv_mode=<span class="literal">None</span>, store_cv_values=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>gcv_mode</strong>：{‘auto’, ‘svd’, eigen’}, default=’auto’</p>
<p>指示执行通用交叉验证时使用哪种策略的标志</p>
</li>
</ul>
<h2 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h2><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso"><code>Lasso</code></a> 类的实现使用了 coordinate descent （坐标下降算法）来拟合系数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line">reg = linear_model.Lasso(alpha=<span class="number">0.1</span>)</span><br><span class="line">reg.fit([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]], [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">print(reg.predict([[<span class="number">1</span>, <span class="number">1</span>]]), reg.coef_, reg.intercept_)</span><br></pre></td></tr></table></figure>

<p>对于较简单的任务，同样有用的是函数 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path"><code>lasso_path</code></a> 。它能够通过搜索所有可能的路径上的值来计算系数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Lasso(alpha=<span class="number">1.0</span>, *, fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, precompute=<span class="literal">False</span>, copy_X=<span class="literal">True</span>, max_iter=<span class="number">1000</span>, tol=<span class="number">0.0001</span>, warm_start=<span class="literal">False</span>, positive=<span class="literal">False</span>, random_state=<span class="literal">None</span>, selection=<span class="string">&#x27;cyclic&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>alpha：</strong>float, optional</li>
</ul>
<p>　　正则项参数。常数。默认值1.0。alpha=0时转化为最小二乘估计，由线性回归模型求解。使用Lasso模型时，通常令alpha≠0。</p>
<ul>
<li><strong>fit_intercept：</strong>boolean, optional, default True</li>
</ul>
<p>　　是否计算截距。如果为False，对数据进行去中心化处理。</p>
<ul>
<li><strong>normalize：</strong>boolean, optional, default False</li>
</ul>
<p>　　对数据X进行标准化</p>
<ul>
<li><strong>precompute：</strong>True | False | array-like, default=False</li>
</ul>
<p>　　是否使用事先计算好的Gram矩阵来加速模型计算。如果precompute=’auto’，让程序自动决定。Gram矩阵可以作为参数被传递。对于稀疏数据，通常令precompute=True，保留稀疏性。</p>
<ul>
<li><strong>copy_X：</strong>boolean, optional, default True</li>
</ul>
<p>　　如果copy_X=True，复制X；如果copy_X=False，覆盖上次运行的X。</p>
<ul>
<li><strong>max_iter：</strong>int, optional</li>
</ul>
<p>　　最大迭代次数。</p>
<ul>
<li><strong>tol：</strong>float, optional</li>
</ul>
<p>　　优化容忍度：如果更新大于tol，继续优化，直到小于tol。</p>
<ul>
<li><strong>warm_start：</strong>bool, optional</li>
</ul>
<p>　　如果warm_start=True，使用上次的解作为初始化；如果warm_start=False，清除之前的解。</p>
<ul>
<li><strong>positive：</strong>bool, optional</li>
</ul>
<p>　　如果positive=True，强制将系数设为正数。</p>
<ul>
<li><p><strong>random_state：</strong>int, RandomState instance or None, optional, default None</p>
<p>伪随机数发生器种子，随机选择特征来更新模型。</p>
</li>
<li><p><strong>selection：</strong>str, default ‘cyclic’</p>
<p>如果为‘random’，每次迭代都会更新随机系数，而不是按顺序遍历每个特征。该参数值可以使得算法更快收敛，尤其当tol&gt;1e-4时。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/29/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" data-id="ckd79u58p0000bkdzcbxr10o2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/07/29/%E5%B2%AD%E5%9B%9E%E5%BD%92%20LASSO%E5%9B%9E%E5%BD%92/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/07/29/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>