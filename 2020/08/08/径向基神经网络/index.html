<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  
  <link rel="stylesheet" href="/lib/animate-css/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"8.0.0-rc.4","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false};
  </script>

  <meta name="description" content="径向基函数，正则化径向基神经网络，广义的RBF神经网络，神经网络的学习方法，RBF神经网络与BP神经网络的区别">
<meta property="og:type" content="article">
<meta property="og:title" content="径向基神经网络">
<meta property="og:url" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Zima Blue&#39;s blog">
<meta property="og:description" content="径向基函数，正则化径向基神经网络，广义的RBF神经网络，神经网络的学习方法，RBF神经网络与BP神经网络的区别">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/6.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/7.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/8.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/9.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/10.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/11.png">
<meta property="og:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/12.png">
<meta property="article:published_time" content="2020-08-07T16:00:00.000Z">
<meta property="article:modified_time" content="2020-08-10T10:31:35.702Z">
<meta property="article:author" content="lyp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.png">

<link rel="canonical" href="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>径向基神经网络 | Zima Blue's blog</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zima Blue's blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">径向基神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%84%E5%90%91%E5%9F%BA%E5%87%BD%E6%95%B0-%E6%8F%92%E5%80%BC%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.</span> <span class="nav-text">径向基函数  插值问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.</span> <span class="nav-text">正则化径向基神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%9A"><span class="nav-number">1.2.1.</span> <span class="nav-text">神经网络的结构：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8E%E6%8F%92%E5%80%BC%E9%97%AE%E9%A2%98%E7%9A%84%E8%81%94%E7%B3%BB%EF%BC%9A"><span class="nav-number">1.2.2.</span> <span class="nav-text">与插值问题的联系：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96RBF%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A"><span class="nav-number">1.2.3.</span> <span class="nav-text">正则化RBF网络的学习算法：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E7%9A%84RBF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.3.</span> <span class="nav-text">广义的RBF神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%9A-1"><span class="nav-number">1.3.1.</span> <span class="nav-text">神经网络的结构：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89RBF%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">1.3.2.</span> <span class="nav-text">广义RBF网络学习方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%9A%8F%E6%9C%BA%E9%80%89%E5%8F%96%E5%9B%BA%E5%AE%9A%E4%B8%AD%E5%BF%83"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">1.随机选取固定中心</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83"><span class="nav-number">1.3.2.1.1.</span> <span class="nav-text">数据中心</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%B7%AE"><span class="nav-number">1.3.2.1.2.</span> <span class="nav-text">标准差</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%90%E5%B1%82%E5%88%B0%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E6%9D%83%E5%80%BC"><span class="nav-number">1.3.2.1.3.</span> <span class="nav-text">隐层到输出层的权值</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%87%AA%E7%BB%84%E7%BB%87%E9%80%89%E5%8F%96%E4%B8%AD%E5%BF%83"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">2.自组织选取中心</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83-1"><span class="nav-number">1.3.2.2.1.</span> <span class="nav-text">数据中心</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%B7%AE-1"><span class="nav-number">1.3.2.2.2.</span> <span class="nav-text">标准差</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%90%E5%B1%82%E5%88%B0%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E6%9D%83%E5%80%BC-1"><span class="nav-number">1.3.2.2.3.</span> <span class="nav-text">隐层到输出层的权值</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%9C%89%E7%9B%91%E7%9D%A3%E9%80%89%E5%8F%96%E4%B8%AD%E5%BF%83"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">3.有监督选取中心</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RBF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8EBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E7%BC%BA%E7%82%B9%E6%AF%94%E8%BE%83"><span class="nav-number">1.4.</span> <span class="nav-text">RBF神经网络与BP神经网络优缺点比较</span></a></li></ol></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">lyp</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </section>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lyp">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zima Blue's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          径向基神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-08 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-08T00:00:00+08:00">2020-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-10 18:31:35" itemprop="dateModified" datetime="2020-08-10T18:31:35+08:00">2020-08-10</time>
              </span>

          
            <div class="post-description">径向基函数，正则化径向基神经网络，广义的RBF神经网络，神经网络的学习方法，RBF神经网络与BP神经网络的区别</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="径向基神经网络"><a href="#径向基神经网络" class="headerlink" title="径向基神经网络"></a>径向基神经网络</h1><p> &emsp;</p>
<h2 id="径向基函数-插值问题"><a href="#径向基函数-插值问题" class="headerlink" title="径向基函数  插值问题"></a>径向基函数  插值问题</h2><p> &emsp;&emsp;1963年Davis提出高维空间的多变量插值理论。径向基函数技术则是20世纪80年代后期，Powell在解决“多变量有限点严格（精确）插值问题”时引入的，目前径向基函数已成为数值分析研究中的一个重要领域。</p>
<p>&emsp;&emsp;考虑一个由N维输入空间到一维输出空间的映射。设N维空间有 $ p $ 个输入向量平，它  $p$=1, 2, …., P们在输出空间相应的目标值为$d^p$，$p$=1, 2, …., P  ，P对输入一输出样本构成了训练样本集。插值的目的是寻找一个非线性映射函数F(X)，使其满足下述插值条件：</p>
<p>&emsp;&emsp;$\mathbf{F(x)=d^p}$           $p$=1, 2, …., P &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>(1)</strong></p>
<p>&emsp;&emsp;式子中，函数$F(x)$描述了一个插值曲面，所谓严格插值或精确插值，是一种完全内插，即该插值曲面必须通过所有训练数据点。</p>
<p>&emsp;&emsp;那么到底什么是插值，在这里简单的解释一下，就是通过训练集数据，我找到一个曲面，这个曲面可以完全覆盖这些训练点，那么找到这个曲面后就可以通过这个曲面取寻找其他的值了，即只知道一些变量所对应的函数值，但不知道函数具体是什么，要得到这个函数。</p>
<p>&emsp;&emsp;采用径向基函数技术解决插值问题的方法是，选择 <strong>P</strong> 个训练数据，各基函数的形式为：</p>
<p>&emsp;&emsp;$\mathbf{φ}(||x-x^p||_2)$        $p$=1, 2, …., P&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>(2)</strong></p>
<p>&emsp;&emsp;式中，基函数$\mathbf{φ}$为非线性函数，训练数据点 $x^p$ 是 $\mathbf{φ}$ 的中心。基函数以输人空间的点 x 与中心的距离  $x^p$ 作为函数的自变量。由于距离是径向同性的，故函数被称为<strong>径向基函数</strong>。<strong>最常用的是高斯核函数</strong>，基于径向基函数技术的插值函数定义为基函数的线性组合：</p>
<p>&emsp;&emsp;$F(x)=\sum_{p=1}^{\mathbf{P}}{\mathbf{ω}_p\mathbf{φ}(||x-x^p||)}$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>(3)</strong></p>
<p>&emsp;&emsp;此公式是选取了 <strong>P</strong> 个数据中心，计算结果是自变量距离各个数据中心的距离按照各自权重相加和。这些数据中心都是在样本中挑选的，一共选取了 <strong>P</strong> 个，有 <strong>P</strong> 个权重，即有 <strong>P</strong> 个未知数，所以需要将所有样本都用上才可以得到各个权重。所以挑选的数据中心的数量不能大于样本数。</p>
<p>&emsp;&emsp;将（1）式的插值条件代入上式，得到 <strong>P</strong> 个关于未知系数$\mathbf{ω}_P$， P=1, 2, …., $p$  的线性方程组：</p>
<p>&emsp;&emsp;$\sum_{p=1}^{\mathbf{P}}{\mathbf{ω}_p\mathbf{φ}(||x^1-x^p||)}=d^1$</p>
<p>&emsp;&emsp;$\sum_{p=1}^{\mathbf{P}}{\mathbf{ω}_p\mathbf{φ}(||x^2-x^p||)}=d^2$</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\mathbf{···}$</p>
<p>&emsp;&emsp;$\sum_{p=1}^{\mathbf{P}}{\mathbf{ω}_p\mathbf{φ}(||x^\mathbf{P}-x^p||)}=d^\mathbf{P}$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>(4)</strong></p>
<p>令 $\mathbf{φ}_{ip}=\mathbf{φ}(||x^i-x^p||),i=1,2,……,\mathbf{P}\mathbf{, }$  $p=1,2,……,\mathbf{P}$ </p>
<p>则上述方程组可改写为：</p>
<p>&emsp;&emsp;<script type="math/tex">\begin{bmatrix}{\mathbf{φ}_{11}}&{\mathbf{φ}_{12}}&{\cdots}&{\mathbf{φ}_{1\mathbf{P}}}\\{\mathbf{φ}_{21}}&{\mathbf{φ}_{22}}&{\cdots}&{\mathbf{φ}_{2\mathbf{P}}}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\{\mathbf{φ}_{\mathbf{P}1}}&{\mathbf{φ}_{\mathbf{P}2}}&{\cdots}&{\mathbf{φ}_{\mathbf{P}\mathbf{P}}}\\ \end{bmatrix}</script>  <script type="math/tex">\begin{bmatrix}{\mathbf{ω}_{1}}\\{\mathbf{ω}_{2}}\\{\vdots}\\ {\mathbf{ω}_{\mathbf{P}}}\\ \end{bmatrix}</script>  <script type="math/tex">\mathbf{=}\begin{bmatrix}{d^1}\\{d^2}\\{\vdots}\\ {d^\mathbf{P}}\\ \end{bmatrix}</script>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>(5)</strong></p>
<p>令$\mathbf{Φ}$表示元素$\mathbf{φ}_{ip}$的PxP阶矩阵，$\mathbf{w}$和$\mathbf{d}$分别表示系数向量和期望输出向量，</p>
<p>（5）以写成下面的向量形式：</p>
<p>&emsp;&emsp;&emsp;&emsp;$\mathbf{Φ}\mathbf{w}\mathbf{=}\mathbf{d}$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>(6)</strong></p>
<p>式中，$\mathbf{Φ}$称为插值矩阵，若$\mathbf{Φ}$为可逆矩阵，就可以从（6）式中解出系数向量$\mathbf{w}$，即</p>
<p>&emsp;&emsp;&emsp;&emsp;$\mathbf{w}\mathbf{=}\mathbf{Φ^{-1}}\mathbf{d}$</p>
<p> &emsp;</p>
<p><strong>高斯径向基函数：</strong></p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.png" alt="2" style="zoom:80%;"></p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3.png" alt="3"></p>
<p> &emsp;&emsp;横轴就是到中心的距离用半径r表示，如上图，我们发现当距离等于0时，径向基函数等于1，距离越近衰减越快，$\mathbf{σ}$ 越小图像越窄。</p>
<p> &emsp;</p>
<h2 id="正则化径向基神经网络"><a href="#正则化径向基神经网络" class="headerlink" title="正则化径向基神经网络"></a>正则化径向基神经网络</h2><h3 id="神经网络的结构："><a href="#神经网络的结构：" class="headerlink" title="神经网络的结构："></a>神经网络的结构：</h3><p>&emsp;&emsp;正则化RBF网络的结构如下图所示。其特点是：网络具有N个输入节点，<strong>P</strong>个隐节点，一个隐节点有一个数据中心，i个输出节点；网络的隐节点数等于输入样本数，隐节点的激活函数常高斯径向基函数，并将所有输入样本设为径向基函数的中心，各径向基函数取统一的扩展常数。</p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.png" alt="2" style="zoom:80%;"></p>
<p> &emsp;&emsp;设输入层的任意节点你用 <strong>i</strong> 表示，隐节点任一节点用 <strong>j</strong> 表示，输出层的任一节点用 <strong>k</strong> 表示。对各层的数学描述如下：</p>
<p>&emsp;&emsp;输入向量：$x=(x_1,x_1,…,x_N)^T$</p>
<p>&emsp;&emsp;任一隐节点的激活函数：$\mathbf{φ}(X,X^P)$ 称为基函数。一般使用高斯函数。</p>
<p>&emsp;&emsp;输出权矩阵：$\mathbf{W}$，其中$\mathbf{ω}_{ik}$，为隐层的第 <strong>j</strong> 个节点与输出层第 <strong>k</strong> 个节点间的突触权值。</p>
<p>&emsp;&emsp;输出向量： $Y=(y_1,y_2,…,y_l)$</p>
<p> &emsp;</p>
<p>&emsp;&emsp;当输人训练集中的某个样本 $x^p$ 时，对应的期望输出 $d^p$ 就是教师信号。为了确定网络隐层到输出层之间的P个权值，需要将训练集中的样本逐一输人一遍，从而可得到式 (4) 中的方程组。网络的权值确定后，对训练集的样本实现了完全内插，即对所有样本误差为0。而对非训练集的输人模式，网络的输出值相当于函数的内插，因此径向基函数网络可用作函数逼近。</p>
<p> &emsp;</p>
<h3 id="与插值问题的联系："><a href="#与插值问题的联系：" class="headerlink" title="与插值问题的联系："></a><strong>与插值问题的联系：</strong></h3><p>&emsp;&emsp;插值问题所要求的函数</p>
<p>&emsp;&emsp;$F(x)=\sum_{p=1}^{\mathbf{P}}{\mathbf{ω}_p\mathbf{φ}(||x-x^p||)}$</p>
<p>&emsp;&emsp;径向基神经网络的结构就是这个函数的体现，隐层的每一个神经元的激活函数就是$\mathbf{φ}(||x-x^p||)$，激活函数得出结果后，每个结果与各自的权值相乘，隐层到输出层的权值就是式中的$\mathbf{ω}_p$，最后相加得到结果。</p>
<p> &emsp;</p>
<h3 id="正则化RBF网络的学习算法："><a href="#正则化RBF网络的学习算法：" class="headerlink" title="正则化RBF网络的学习算法："></a>正则化RBF网络的学习算法：</h3><p>&emsp;&emsp; 当采用正则化RBP网络结构时，隐节点数即样本数，基函数的数据中心即为样本本身，只需考虑扩展常数和输出节点的权值。径向基函数的扩展常数可根据数据中心的散布而确定，为了避免每个径向基函数太尖或太平，一种选择方法是将所有径向基函数的扩展常数设为：</p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4.png" alt="4" style="zoom:80%;"></p>
<p>&emsp;&emsp;式中，$d_{max}$是样本之间的最大距离，P是样本的数目。</p>
<p>&emsp;&emsp; 隐层到输出层的权值采用梯度下降法。</p>
<p> &emsp;</p>
<h2 id="广义的RBF神经网络"><a href="#广义的RBF神经网络" class="headerlink" title="广义的RBF神经网络"></a>广义的RBF神经网络</h2><p>&emsp;&emsp;正则化的RBF要求所有样本对应一个隐层神经元，所带来额问题是计算量很大，因为一旦样本成千上万则计算量急剧增加，计算量会急剧加大，为解决这一问题，可减少隐层节点的个数。广义的RBF神经网络的隐层节点不会随着样本数的增加而增加，而是N&lt;M&lt;P，N为样本维数，M为隐层节点数，P为样本个数。</p>
<h3 id="神经网络的结构：-1"><a href="#神经网络的结构：-1" class="headerlink" title="神经网络的结构："></a>神经网络的结构：</h3><p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5.png" alt="5" style="zoom: 67%;"></p>
<p>&emsp;&emsp;与正则化RBF神经网络相比多了阈值 $\mathbf{φ_0}$ ，其输出恒为1，少了隐层神经元的数量</p>
<p>&emsp;&emsp;网络第 <strong>j</strong> 个输出神经元得出的结果是：</p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/6.png" alt="6"></p>
<p> &emsp;</p>
<p>&emsp;&emsp;从输入层到隐藏层相当于是把低维空间的数据映射到高维空间，输入层细胞个数为样本的维度，所以隐藏层细胞个数一定要比输入层细胞个数多。从隐藏层到输出层是对高维空间的数据进行线性分类的过程，可以采用单层感知器常用的那些学习规则。</p>
<p>&emsp;&emsp;注意广义RBF网络只要求隐藏层神经元个数大于输入层神经元个数，并没有要求等于输入样本个数，实际上它比样本数目要少得多。因为在标准RBF网络中，当样本数目很大时，就需要很多基函数，权值矩阵就会很大，计算复杂且容易产生病态问题。另外广RBF网与传统RBF网相比，还有以下不同：</p>
<ol>
<li><p>径向基函数的中心不再限制在输入数据点上，而由训练算法确定。</p>
</li>
<li><p>各径向基函数的扩展常数不再统一，而由训练算法确定。</p>
</li>
<li><p>输出函数的线性变换中包含阈值参数，用于补偿基函数在样本集上的平均值与目标值之间的差别。</p>
</li>
</ol>
<p>因此广义RBF网络的设计包括：</p>
<p>&emsp;&emsp;结构设计—隐藏层含有几个节点合适</p>
<p>&emsp;&emsp;参数设计—各基函数的数据中心及扩展常数、输出节点的权值。</p>
<p> &emsp;</p>
<h3 id="广义RBF网络学习方法"><a href="#广义RBF网络学习方法" class="headerlink" title="广义RBF网络学习方法"></a>广义RBF网络学习方法</h3><p> &emsp;</p>
<h4 id="1-随机选取固定中心"><a href="#1-随机选取固定中心" class="headerlink" title="1.随机选取固定中心"></a>1.随机选取固定中心</h4><h5 id="数据中心"><a href="#数据中心" class="headerlink" title="数据中心"></a><strong>数据中心</strong></h5><p>&emsp;&emsp;数据中心从样本输入中选取，样本密集的地方中心点可以适当多些，样本稀疏的地方中心点可以少些，若数据本身是均匀分布的，中心点也可以均匀分布。总之，选出的数据中心应具有代表性。</p>
<h5 id="标准差"><a href="#标准差" class="headerlink" title="标准差"></a>标准差</h5><p>&emsp;&emsp;径向基函数的标准差是根据数据中心的散布而确定的，为了避免每个径向基函数太尖或太平，一种选择方法是将所有径向基函数的标准差设为：</p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/7.png" alt="7" style="zoom:67%;"></p>
<p>&emsp;&emsp;式中，$d_{max}$是样本之间的最大距离，M是数据中心的数目。</p>
<h5 id="隐层到输出层的权值"><a href="#隐层到输出层的权值" class="headerlink" title="隐层到输出层的权值"></a>隐层到输出层的权值</h5><p>&emsp;&emsp;可以使用<strong>LMS</strong>（只使用一个样本的梯度下降）和<strong>伪逆法</strong></p>
<p> &emsp;</p>
<h4 id="2-自组织选取中心"><a href="#2-自组织选取中心" class="headerlink" title="2.自组织选取中心"></a>2.自组织选取中心</h4><h5 id="数据中心-1"><a href="#数据中心-1" class="headerlink" title="数据中心"></a><strong>数据中心</strong></h5><p>&emsp;&emsp;数据中心的自组织选择常采用各种动态<strong>聚类算法</strong>对数据中心进行自组织选择，在学习过程中需对数据中心的位置进行动态调节。除了聚类，也可以使用<strong>梯度训练法</strong>、<strong>资源分配网络</strong>等。</p>
<h5 id="标准差-1"><a href="#标准差-1" class="headerlink" title="标准差"></a><strong>标准差</strong></h5><p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/8.png" alt="8"></p>
<p>&emsp;&emsp;对于每一个数据中心，先找出距离这个数据中心最近的另一个数据中心，标准差就是这两个数据中心之间距离的$\mathbf{λ}$倍，λ是重叠系数。</p>
<h5 id="隐层到输出层的权值-1"><a href="#隐层到输出层的权值-1" class="headerlink" title="隐层到输出层的权值"></a><strong>隐层到输出层的权值</strong></h5><p><strong>&emsp;&emsp;LMS</strong>或<strong>伪逆法</strong></p>
<p> &emsp;</p>
<h4 id="3-有监督选取中心"><a href="#3-有监督选取中心" class="headerlink" title="3.有监督选取中心"></a>3.有监督选取中心</h4><p>&emsp;&emsp;RBF函数中心、扩展常数、输出权值都应该采用监督学习算法进行训练，经历一个误差修正学习的过程，与BP网络的学习原理一样。同样采用<strong>梯度下降法</strong>，定义目标函数为</p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/9.png" alt="9"></p>
<p>&emsp;&emsp;$e_i$为输入第i个样本时的误差信号。</p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/10.png" alt="10"></p>
<p>&emsp;&emsp;上式的输出函数中忽略了阈值。</p>
<p>&emsp;&emsp;为使目标函数最小化，各参数的修正量应与其负梯度成正比，即</p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/11.png" alt="11"></p>
<p>&emsp;&emsp;c 为数据中心，$\mathbf{σ}$ 为标准差，$w$ 为权值。</p>
<p>&emsp;&emsp;具体计算式为</p>
<p><img src="/2020/08/08/%E5%BE%84%E5%90%91%E5%9F%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/12.png" alt="12"></p>
<p> &emsp;</p>
<h2 id="RBF神经网络与BP神经网络优缺点比较"><a href="#RBF神经网络与BP神经网络优缺点比较" class="headerlink" title="RBF神经网络与BP神经网络优缺点比较"></a>RBF神经网络与BP神经网络优缺点比较</h2><ol>
<li><p>RBF 的泛化能力在多个方面都优于BP 网络，但是在解决具有相同精度要求的问题时，BP 网络的结构要比RBF <strong>网络简单</strong>。</p>
</li>
<li><p>RBF 网络的<strong>逼近精度</strong>要明显高于BP 网络，它几乎能实现完全逼近，而且设计起来极其方便, 网络可以自动增加神经元直到满足精度要求为止。但是在训练样本增多时，RBF 网络的隐层神经元数远远高于前者，使得RBF 网络的复杂度大增加，结构过于庞大，从而<strong>运算量也有所增加</strong>。</p>
</li>
<li>RBF神经网络是一种性能优良的前馈型神经网络，RBF网络可以任意精度逼近任意的非线性函数，且具有<strong>全局逼近能力</strong>，从根本上解决了BP网络的<strong>局部最优问题</strong>，而且拓扑结构紧凑，结构参数可实现分离学习，<strong>收敛速度快</strong>。</li>
<li><p>bp神经网络学习速率是固定的，因此网络的<strong>收敛速度慢</strong>，需要较长的训练时间。对于一些复杂问题，BP算法需要的训练时间可能非常长，而rbf神经网络是种高效的前馈式网络，它具有其他前向网络所不具有的最佳逼近性能和全局最优特性，并且结构简单，<strong>训练速度快</strong>。</p>
<p>&emsp;</p>
</li>
</ol>
<p><strong>RBF网络的优点：</strong></p>
<p>①  它具有唯一最佳逼近的特性，且无局部极小问题存在。</p>
<p>②  RBF神经网络具有较强的输入和输出映射功能，并且理论证明在前向网络中RBF网络是完成映射功能的最优网络。</p>
<p>③  网络连接权值与输出呈线性关系。</p>
<p>④  分类能力好。</p>
<p>⑤  学习过程收敛速度快。</p>
<p>&emsp;&emsp; </p>
<p>&emsp;&emsp; </p>
<p>&emsp;&emsp; </p>
<p>&emsp;&emsp; </p>
<p>径向基核函数：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42398658/article/details/83215916">https://blog.csdn.net/weixin_42398658/article/details/83215916</a></p>
<p>径向基神经网络：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42398658/article/details/84342747">https://blog.csdn.net/weixin_42398658/article/details/84342747</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zb1165048017/article/details/49385359">https://blog.csdn.net/zb1165048017/article/details/49385359</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lin_angel/article/details/50725600">https://blog.csdn.net/lin_angel/article/details/50725600</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_29840153/article/details/92760646">https://blog.csdn.net/qq_29840153/article/details/92760646</a></p>
<p>RBF与BP神经网络比较</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sunxinyu/article/details/76598446">https://blog.csdn.net/sunxinyu/article/details/76598446</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/08/07/%E6%A0%B8%E5%87%BD%E6%95%B0/" rel="prev" title="核函数">
      <i class="fa fa-chevron-left"></i> 核函数
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/08/19/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8E%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/" rel="next" title="拉格朗日乘子法与对偶问题">
      拉格朗日乘子法与对偶问题 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lyp</span>
</div>
<span id="busuanzi_container_site_uv">
  访客数<span id="busuanzi_value_site_uv"></span>人次
</span>
        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      const script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
<!--动态线条背景-->
<script type="text/javascript"
color="150,150,150" opacity='1' zIndex="-2" count="180" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
</script>
